{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "Apply `BatchNorm`, `Dropout` and `Residual` on MLP networks for CIFAR-10 classification.\n",
    "\n",
    "For BatchNorm and Dropout, design models with `BatchNorm Layer`, `Dropout Layer` and both the layers. Compare the results with a plain MLP, and with each other.\n",
    "\n",
    "For Residual, design a simple `Redisual Block` based on a deeper MLP. Compare the results and see whether adding residual works.\n",
    "\n",
    "Follow the pipeline in your Homework 2 to finish model designing, training and testing.\n",
    "\n",
    "### Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "# 设置 matplotlib 显示格式\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)  # 设置默认图像大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# CIFAR-10 数据集的下载和加载\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # 转换为 Tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]  # 归一化\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "class OursDatasetwithTransforms(Dataset):\n",
    "    def __init__(self, data, labels, transforms):\n",
    "        \"\"\"\n",
    "        初始化数据集\n",
    "        data: 输入数据，例如一个 NumPy 数组或 PyTorch 张量\n",
    "        labels: 对应的标签\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        # 每个Dataset都必须写，返回数据集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 每个Dataset都必须写，获取指定索引idx的数据和标签\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        sample = self.transforms(sample)\n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "        \n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),  # [0, 255], PIL Image / ndarray --> [0, 1], torch.tensor\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]  # 归一化, (input[channel] - mean[channel]) / std[channel]\n",
    ")\n",
    "traindata = trainset.data[:5000]\n",
    "trainlabels = trainset.targets[:5000]\n",
    "testdata = testset.data[:500]\n",
    "testlabels = testset.targets[:500]\n",
    "\n",
    "# 初始化自定义数据集\n",
    "trainset = OursDatasetwithTransforms(traindata, trainlabels, transform)\n",
    "testset = OursDatasetwithTransforms(testdata, testlabels, transform)\n",
    "trainloader = DataLoader(trainset, batch_size=256, shuffle=False, drop_last=False)\n",
    "testloader = DataLoader(testset, batch_size=256, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for plotting the loss values\n",
    "from typing import List\n",
    "def plot_loss(num_epochs: int, train_losses: List, test_losses: List) -> None:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss', marker='o')\n",
    "    plt.plot(range(1, num_epochs + 1), test_losses, label='Testing Loss', marker='x')\n",
    "    plt.title('Training and Testing Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design a 3-Layer MLP with BatchNorm and Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Here is the simple version of A 3-layer MLP\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=512):\n",
    "        super(SimpleMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # For CIFAR-10, input size is 32x32x3\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)  # Output size is 10 classes for CIFAR-10\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)  # Flatten the image\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Implementation of MLP with BatchNorm only\n",
    "# Use nn.BatchNorm1d\n",
    "class MLPWithBatchNorm(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=512):\n",
    "        # your code here\n",
    "        super(MLPWithBatchNorm, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)  # BatchNorm layer1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)  # BatchNorm layer2\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)  # use BatchNorm\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)  # use BatchNorm\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Implementation of MLP with Dropout only \n",
    "# Use nn.Dropout\n",
    "class MLPWithDropout(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=512, dropout_prob=0.5):\n",
    "        # your code here\n",
    "        super(MLPWithDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # use Dropout\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)  # use Dropout\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "# Implementation of MLP with both BatchNorm and Dropout\n",
    "# Use nn.BatchNorm1d and nn.Dropout  \n",
    "class MLPWithBatchNormDropout(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, hidden_size=512, dropout_prob=0.5):\n",
    "        # your code here\n",
    "        super(MLPWithBatchNormDropout, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(hidden_size)  # BatchNorm layer1\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(hidden_size)  # BatchNorm layer2\n",
    "        self.fc3 = nn.Linear(hidden_size, 10)\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)  # Dropout layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 32 * 32 * 3)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)  # use BatchNorm\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # use Dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)  # use BatchNorm\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)  # use Dropout\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Testing Loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(model, train_loader):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_acc = 100. * correct / total\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test(model, test_loader):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    test_loss = running_loss / len(test_loader)\n",
    "    test_acc = 100. * correct / total\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charles0618\\AppData\\Local\\Temp\\ipykernel_17784\\962370680.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.6402, Train Acc: 42.03%, Test Loss: 1.4854, Test Acc: 48.4%\n",
      "Epoch 2, Train Loss: 1.4112, Train Acc: 50.60%, Test Loss: 1.3871, Test Acc: 52.8%\n",
      "Epoch 3, Train Loss: 1.2760, Train Acc: 55.45%, Test Loss: 1.3653, Test Acc: 54.6%\n",
      "Epoch 4, Train Loss: 1.1606, Train Acc: 59.49%, Test Loss: 1.3706, Test Acc: 53.8%\n",
      "Epoch 5, Train Loss: 1.0544, Train Acc: 63.14%, Test Loss: 1.3841, Test Acc: 52.8%\n",
      "Epoch 6, Train Loss: 0.9507, Train Acc: 66.95%, Test Loss: 1.4440, Test Acc: 51.4%\n",
      "Epoch 7, Train Loss: 0.8582, Train Acc: 70.28%, Test Loss: 1.4713, Test Acc: 51.6%\n",
      "Epoch 8, Train Loss: 0.7781, Train Acc: 73.35%, Test Loss: 1.5333, Test Acc: 52.8%\n",
      "Epoch 9, Train Loss: 0.7145, Train Acc: 75.54%, Test Loss: 1.6688, Test Acc: 50.4%\n",
      "Epoch 10, Train Loss: 0.6499, Train Acc: 77.74%, Test Loss: 1.7774, Test Acc: 48.8%\n"
     ]
    }
   ],
   "source": [
    "# Train SimpleMLP\n",
    "# your code here\n",
    "def train_with_test(model, trainloader, testloader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train(model, trainloader)\n",
    "        test_loss, test_acc = test(model, testloader)\n",
    "        print(f'Epoch {epoch + 1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%,',\n",
    "            f'Test Loss: {test_loss:.4f}, Test Acc: {test_acc}%')\n",
    "    \n",
    "model_simple = SimpleMLP().to(device)\n",
    "train_with_test(model_simple, trainloader, testloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charles0618\\AppData\\Local\\Temp\\ipykernel_17784\\962370680.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.5909, Train Acc: 43.49%, Test Loss: 1.4309, Test Acc: 49.2%\n",
      "Epoch 2, Train Loss: 1.3737, Train Acc: 51.31%, Test Loss: 1.3606, Test Acc: 52.8%\n",
      "Epoch 3, Train Loss: 1.2425, Train Acc: 56.04%, Test Loss: 1.3414, Test Acc: 52.8%\n",
      "Epoch 4, Train Loss: 1.1370, Train Acc: 59.90%, Test Loss: 1.3258, Test Acc: 53.4%\n",
      "Epoch 5, Train Loss: 1.0407, Train Acc: 63.52%, Test Loss: 1.3432, Test Acc: 53.0%\n",
      "Epoch 6, Train Loss: 0.9485, Train Acc: 67.09%, Test Loss: 1.3591, Test Acc: 53.6%\n",
      "Epoch 7, Train Loss: 0.8579, Train Acc: 70.53%, Test Loss: 1.3855, Test Acc: 54.8%\n",
      "Epoch 8, Train Loss: 0.7727, Train Acc: 73.72%, Test Loss: 1.4505, Test Acc: 53.0%\n",
      "Epoch 9, Train Loss: 0.6964, Train Acc: 76.76%, Test Loss: 1.5497, Test Acc: 52.2%\n",
      "Epoch 10, Train Loss: 0.6284, Train Acc: 79.15%, Test Loss: 1.6411, Test Acc: 52.0%\n"
     ]
    }
   ],
   "source": [
    "# Train MLPWithBatchNorm\n",
    "model_with_BN = MLPWithBatchNorm().to(device)\n",
    "train_with_test(model_with_BN, trainloader, testloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charles0618\\AppData\\Local\\Temp\\ipykernel_17784\\962370680.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.8065, Train Acc: 35.53%, Test Loss: 1.5994, Test Acc: 44.4%\n",
      "Epoch 2, Train Loss: 1.6568, Train Acc: 41.06%, Test Loss: 1.5426, Test Acc: 44.8%\n",
      "Epoch 3, Train Loss: 1.6018, Train Acc: 43.50%, Test Loss: 1.4990, Test Acc: 47.0%\n",
      "Epoch 4, Train Loss: 1.5621, Train Acc: 44.94%, Test Loss: 1.4766, Test Acc: 51.2%\n",
      "Epoch 5, Train Loss: 1.5308, Train Acc: 45.91%, Test Loss: 1.4633, Test Acc: 49.8%\n",
      "Epoch 6, Train Loss: 1.5114, Train Acc: 46.84%, Test Loss: 1.4397, Test Acc: 48.6%\n",
      "Epoch 7, Train Loss: 1.4801, Train Acc: 47.90%, Test Loss: 1.4349, Test Acc: 48.2%\n",
      "Epoch 8, Train Loss: 1.4618, Train Acc: 48.35%, Test Loss: 1.4153, Test Acc: 49.2%\n",
      "Epoch 9, Train Loss: 1.4443, Train Acc: 48.89%, Test Loss: 1.4274, Test Acc: 50.6%\n",
      "Epoch 10, Train Loss: 1.4255, Train Acc: 49.70%, Test Loss: 1.4048, Test Acc: 50.2%\n"
     ]
    }
   ],
   "source": [
    "# Training MLPWithDropout\n",
    "# your code here\n",
    "model_with_Dropout = MLPWithDropout().to(device)\n",
    "train_with_test(model_with_Dropout, trainloader, testloader, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\charles0618\\AppData\\Local\\Temp\\ipykernel_17784\\962370680.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 1.7823, Train Acc: 35.85%, Test Loss: 1.5633, Test Acc: 44.6%\n",
      "Epoch 2, Train Loss: 1.6027, Train Acc: 42.60%, Test Loss: 1.4736, Test Acc: 48.2%\n",
      "Epoch 3, Train Loss: 1.5246, Train Acc: 45.50%, Test Loss: 1.4158, Test Acc: 50.0%\n",
      "Epoch 4, Train Loss: 1.4729, Train Acc: 47.28%, Test Loss: 1.3857, Test Acc: 51.2%\n",
      "Epoch 5, Train Loss: 1.4282, Train Acc: 48.96%, Test Loss: 1.3694, Test Acc: 50.8%\n",
      "Epoch 6, Train Loss: 1.3981, Train Acc: 50.14%, Test Loss: 1.3248, Test Acc: 53.6%\n",
      "Epoch 7, Train Loss: 1.3688, Train Acc: 51.08%, Test Loss: 1.3226, Test Acc: 52.8%\n",
      "Epoch 8, Train Loss: 1.3413, Train Acc: 52.02%, Test Loss: 1.3009, Test Acc: 52.8%\n",
      "Epoch 9, Train Loss: 1.3166, Train Acc: 52.89%, Test Loss: 1.2798, Test Acc: 52.2%\n",
      "Epoch 10, Train Loss: 1.2878, Train Acc: 54.06%, Test Loss: 1.2813, Test Acc: 52.4%\n"
     ]
    }
   ],
   "source": [
    "# Training MLPWithBatchNormDropout\n",
    "# your code here\n",
    "model_with_BN_Dropout = MLPWithBatchNormDropout().to(device)\n",
    "train_with_test(model_with_BN_Dropout, trainloader, testloader, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Analysis the results and find out whether any of the above techniques is useful for training this MLP for CIFAR-10 classification. Explain why/how they work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your answer:\n",
    "\n",
    "Regard `SimpleMLP()` as the baseline, it can be seen from the training output above that the three advanced models did   improve the classification result.\n",
    "\n",
    "- As for `MLPWithBatchNorm()` , 2 `BatchNorm` layers are incorporated. By standardizing the output of the hidden layer, training was accelerated and the model stability can be improved. We can see that after 10 epochs of training, both Train and Test loss were lower, both accuracy were higher compared to the `SimpleMLP()` (those improvements were slight, though).\n",
    "- As for `MLPWithBatchNorm()` , `Dropout` is used after activation. By randomly discarding some neurons, overfitting can be effectively reduced and the model's generalization ability can be increased. We can see that the Train accuracy was obviously lower than `SimpleMLP()` , indicating less overfitting. Also, the Test accuracy was higher, slightly though.\n",
    "- As for `MLPWithBatchNormDropout()` , it combines the advantages of `MLPWithBatchNorm()` and `MLPWithBatchNorm()` , so its training result showed a better generalization ability on Test dataloader and a lower overfitting level on Train dataloader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Design a MLP that has deeper layers with Residual Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deeper MLP with 50 layers\n",
    "class DeepMLP(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, num_classes=10, hidden_size=512, num_layers=50):\n",
    "        super(DeepMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        # Stack multiple hidden layers\n",
    "        self.hidden_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.bn_layers = nn.ModuleList([\n",
    "            nn.BatchNorm1d(hidden_size) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.view(-1, 32*32*3)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "\n",
    "        # Pass through deep hidden layers\n",
    "        for i in range(len(self.hidden_layers)):\n",
    "            x = F.relu(self.bn_layers[i](self.hidden_layers[i](x)))\n",
    "        \n",
    "        # Output layer for classification\n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design a residual block\n",
    "# input -> Linear -> batchnorm -> activation -> dropout -> Linear -> batchnorm -> skip connection -> activation -> output\n",
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_prob=0.5):\n",
    "        # your code here\n",
    "        pass\n",
    "\n",
    "# Design MLP with residual blocks\n",
    "# Use nn.ModuleList \n",
    "# input -> Linear -> batchnorm -> activation -> residual layers -> classifacation layer\n",
    "class DeepResidualMLP(nn.Module):\n",
    "    def __init__(self, input_size=32*32*3, num_classes=10, hidden_size=512, num_layers=50, dropout_prob=0.5):\n",
    "        # your code here\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DeepMLP\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Save the losses during training and testing and plot them using plot_loss()\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DeepMLP\n",
    "# your code here\n",
    "\n",
    "\n",
    "# Save the losses during training and testing and plot them using plot_loss()\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus: Try to see the gradient flow when training the above networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpuPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
